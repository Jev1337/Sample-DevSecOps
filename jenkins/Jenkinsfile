pipeline {
    agent any

    environment {
        REGISTRY = 'localhost:32000'
        IMAGE_NAME = 'flask-k8s-app'
        TAG = "build-${env.BUILD_NUMBER}"
        SONAR_HOST_URL = "http://sonarqube-sonarqube.sonarqube:9000"
        SONAR_PROJECT_KEY = "flask-k8s-devsecops"
        SONAR_TOKEN = credentials('SONAR_TOKEN')
    }

    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }

        stage('Install Dependencies') {
            steps {
                dir('app') {
                    sh '''
                        python -m venv venv
                        . venv/bin/activate
                        python -m pip install --upgrade pip
                        python -m pip install -r requirements.txt
                        python -m pip install coverage pytest-cov
                    '''
                }
            }
        }

        stage('Run Tests') {
            steps {
                dir('app') {
                    sh '''
                        . venv/bin/activate
                        python -m pytest tests/ -v \
                            --cov=. \
                            --cov-report=xml \
                            --cov-report=html \
                            --junitxml=test-results.xml
                    '''
                }
            }
            post {
                always {
                    archiveArtifacts artifacts: 'app/htmlcov/**/*', allowEmptyArchive: true
                    junit 'app/test-results.xml'
                }
            }
        }
        stage('SonarQube Analysis') {
            steps {
                dir('app') {
                    sh '''
                        # Activate virtual environment
                        . venv/bin/activate
                        
                        # Run SonarQube analysis (tests and coverage already generated in previous stage)
                        sonar-scanner \
                            -Dsonar.projectKey=${SONAR_PROJECT_KEY} \
                            -Dsonar.sources=. \
                            -Dsonar.tests=tests \
                            -Dsonar.host.url=${SONAR_HOST_URL} \
                            -Dsonar.token=${SONAR_TOKEN} \
                            -Dsonar.python.coverage.reportPaths=coverage.xml \
                            -Dsonar.python.xunit.reportPath=test-results.xml \
                            -Dsonar.exclusions=**/*_test.py,**/test_*.py,**/__pycache__/**,**/venv/**
                    '''
                }
            }
            post {
                always {
                    archiveArtifacts artifacts: 'app/coverage.xml', allowEmptyArchive: true
                }
            }
        }

        stage('Trivy FS Scan') {
            steps {
                sh "trivy fs --format table -o trivy-fs-report.txt --severity HIGH,CRITICAL ."
                archiveArtifacts artifacts: 'trivy-fs-report.txt', allowEmptyArchive: true
            }
        }

        stage('Build & Push Image') {
            steps {
                script {
                    def fullImageName = "${env.REGISTRY}/${env.IMAGE_NAME}:${env.TAG}"
                    def buildNumber = env.BUILD_NUMBER
                    
                    // Get current git info
                    def gitCommit = sh(script: 'git rev-parse HEAD', returnStdout: true).trim()
                    def gitUrl = sh(script: 'git config --get remote.origin.url', returnStdout: true).trim()
                    
                    // Debug: show the git URL
                    echo "Git URL: ${gitUrl}"
                    echo "Git Commit: ${gitCommit}"
                    
                    sh """
                        # Create and run Kaniko build job using Python Kubernetes API
                        python3 << 'PYEOF'
import time
import yaml
from kubernetes import client, config

# Load in-cluster config
config.load_incluster_config()

# Create API clients
batch_v1 = client.BatchV1Api()
v1 = client.CoreV1Api()

# Use git clone approach instead of direct git:// context
git_url = "${gitUrl}"
git_commit = "${gitCommit}"

print(f"Git URL: {git_url}")
print(f"Git Commit: {git_commit}")

# For MicroK8s registry, use the host IP instead of localhost
registry_address = "localhost:32000"
# Try to use host.docker.internal or the actual cluster registry service
cluster_registry = "registry.container-registry.svc.cluster.local:5000"
full_image_name = "${fullImageName}"

print(f"Original image name: {full_image_name}")

# Replace localhost with cluster-accessible address
if "localhost:32000" in full_image_name:
    # Use the host network approach
    cluster_image_name = full_image_name.replace("localhost:32000", "registry.container-registry.svc.cluster.local:5000")
    print(f"Cluster image name: {cluster_image_name}")
else:
    cluster_image_name = full_image_name

# Job manifest with git clone initContainer
job_manifest = {
    'apiVersion': 'batch/v1',
    'kind': 'Job',
    'metadata': {
        'name': 'kaniko-build-${buildNumber}',
        'namespace': 'jenkins'
    },
    'spec': {
        'ttlSecondsAfterFinished': 300,
        'template': {
            'spec': {
                'restartPolicy': 'Never',
                'hostNetwork': True,  # Use host network to access localhost:32000
                'initContainers': [{
                    'name': 'git-clone',
                    'image': 'alpine/git:latest',
                    'command': ['sh', '-c'],
                    'args': [f'git clone {git_url} /workspace && cd /workspace && git checkout {git_commit}'],
                    'volumeMounts': [{
                        'name': 'workspace',
                        'mountPath': '/workspace'
                    }]
                }],
                'containers': [{
                    'name': 'kaniko',
                    'image': 'gcr.io/kaniko-project/executor:latest',
                    'args': [
                        '--context=/workspace/app',
                        '--dockerfile=/workspace/app/Dockerfile',
                        f'--destination={full_image_name}',
                        '--insecure',
                        '--skip-tls-verify',
                        '--skip-push-permission-check',
                        '--verbosity=info',
                        '--cache=false',
                        '--cleanup',
                        f'--build-arg=BUILD_DATE={int(time.time())}',
                        f'--build-arg=GIT_COMMIT={git_commit[:8]}'
                    ],
                    'volumeMounts': [{
                        'name': 'workspace',
                        'mountPath': '/workspace'
                    }]
                }],
                'volumes': [{
                    'name': 'workspace',
                    'emptyDir': {}
                }]
            }
        }
    }
}

print("Creating Kaniko build job...")
try:
    # Create the job
    job = batch_v1.create_namespaced_job(namespace='jenkins', body=job_manifest)
    print(f"Job created: {job.metadata.name}")
    
    # Wait for job completion
    print("Waiting for build job to complete...")
    for i in range(24):  # 4 minute timeout (reduced from 10 minutes)
        try:
            job_status = batch_v1.read_namespaced_job(name='kaniko-build-${buildNumber}', namespace='jenkins')
            
            # Show job progress
            active_jobs = job_status.status.active or 0
            failed_jobs = job_status.status.failed or 0
            succeeded_jobs = job_status.status.succeeded or 0
            
            print(f"Job status - Active: {active_jobs}, Failed: {failed_jobs}, Succeeded: {succeeded_jobs}")
            
            # If we have multiple failures, fail fast and show logs
            if failed_jobs >= 2:
                print(f"‚ùå Multiple pod failures detected ({failed_jobs} failures)")
                pods = v1.list_namespaced_pod(namespace='jenkins', label_selector='job-name=kaniko-build-${buildNumber}')
                if pods.items:
                    # Show logs from first failed pod
                    for pod in pods.items:
                        if pod.status.phase == 'Failed':
                            pod_name = pod.metadata.name
                            print(f"FAILURE LOGS from {pod_name}:")
                            try:
                                logs = v1.read_namespaced_pod_log(name=pod_name, namespace='jenkins')
                                print(logs)
                            except Exception as log_error:
                                print(f"Could not get logs: {log_error}")
                            break
                
                # Let ttlSecondsAfterFinished handle cleanup for failed jobs
                print("Multiple failures detected - job will be automatically cleaned up in 5 minutes")
                exit(1)
            
            # Show pod status too
            pods = v1.list_namespaced_pod(namespace='jenkins', label_selector='job-name=kaniko-build-${buildNumber}')
            if pods.items:
                for pod in pods.items:
                    pod_name = pod.metadata.name
                    pod_phase = pod.status.phase
                    print(f"Pod {pod_name}: {pod_phase}")
                    
                    # If pod failed, show error logs immediately
                    if pod_phase == 'Failed':
                        try:
                            logs = v1.read_namespaced_pod_log(name=pod_name, namespace='jenkins')
                            print(f"ERROR LOGS from {pod_name}:")
                            if logs:
                                print(logs[-1000:])  # Show last 1000 chars
                            else:
                                print("  No logs available")
                        except Exception as log_error:
                            print(f"Could not get error logs: {log_error}")
                    
                    # If pod is running, show recent logs
                    elif pod_phase == 'Running' and i > 1:  # Check logs earlier
                        try:
                            logs = v1.read_namespaced_pod_log(name=pod_name, namespace='jenkins', tail_lines=3)
                            print(f"Recent logs from {pod_name}:")
                            if logs:
                                log_lines = logs.split("\\n")
                                for line in log_lines[-2:]:
                                    if line.strip():
                                        print(f"  {line}")
                        except Exception as log_error:
                            print(f"Could not get logs: {log_error}")
            
            if job_status.status.conditions:
                for condition in job_status.status.conditions:
                    if condition.type == 'Complete' and condition.status == 'True':
                        print("‚úÖ Build completed successfully")
                        
                        # Get and show logs
                        pods = v1.list_namespaced_pod(namespace='jenkins', label_selector='job-name=kaniko-build-${buildNumber}')
                        if pods.items:
                            pod_name = pods.items[0].metadata.name
                            print("Build logs:")
                            try:
                                logs = v1.read_namespaced_pod_log(name=pod_name, namespace='jenkins', tail_lines=10)
                                print(logs)
                            except Exception as log_error:
                                print(f"Could not get logs: {log_error}")
                        
                        # Let ttlSecondsAfterFinished handle cleanup automatically
                        print("Job will be automatically cleaned up in 5 minutes (ttlSecondsAfterFinished)")
                        
                        print("Successfully built and pushed ${fullImageName}")
                        exit(0)
                        
                    elif condition.type == 'Failed' and condition.status == 'True':
                        print("‚ùå Build failed")
                        
                        # Get and show error logs
                        pods = v1.list_namespaced_pod(namespace='jenkins', label_selector='job-name=kaniko-build-${buildNumber}')
                        if pods.items:
                            pod_name = pods.items[0].metadata.name
                            print("Error logs:")
                            try:
                                logs = v1.read_namespaced_pod_log(name=pod_name, namespace='jenkins')
                                print(logs)
                            except Exception as log_error:
                                print(f"Could not get logs: {log_error}")
                        
                        # Let ttlSecondsAfterFinished handle cleanup automatically
                        print("Failed job will be automatically cleaned up in 5 minutes")
                        
                        exit(1)
            
            print(f"Waiting for job completion... (attempt {i+1}/24)")
            time.sleep(10)
            
        except Exception as e:
            print(f"Error checking job status: {e}")
            time.sleep(5)  # Shorter sleep on error
    
    print("Job timed out after 4 minutes - will be automatically cleaned up in 5 minutes")
    exit(1)
    
except Exception as e:
    print(f"Failed to create build job: {e}")
    exit(1)

PYEOF
                    """
                }
            }
        }

        stage('Trivy Image Scan') {
            steps {
                script {
                    def fullImageName = "${env.REGISTRY}/${env.IMAGE_NAME}:${env.TAG}"
                    def buildNumber = env.BUILD_NUMBER
                    
                    sh """
                        # Run Trivy scan in a separate pod with host network access
                        python3 << 'PYEOF'
import time
from kubernetes import client, config

# Load in-cluster config
config.load_incluster_config()

# Create API clients
v1 = client.CoreV1Api()

# Create Trivy scan pod with host network
trivy_pod = {
    'apiVersion': 'v1',
    'kind': 'Pod',
    'metadata': {
        'name': 'trivy-scan-${buildNumber}',
        'namespace': 'jenkins'
    },
    'spec': {
        'restartPolicy': 'Never',
        'hostNetwork': True,  # Access to localhost:32000
        'containers': [{
            'name': 'trivy',
            'image': 'aquasec/trivy:latest',
            'command': ['trivy'],
            'args': [
                'image',
                '--format', 'table',
                '--severity', 'HIGH,CRITICAL',
                '--insecure',
                '--timeout', '5m',
                '${fullImageName}'
            ],
            'volumeMounts': [{
                'name': 'scan-results',
                'mountPath': '/results'
            }]
        }],
        'volumes': [{
            'name': 'scan-results',
            'emptyDir': {}
        }]
    }
}

print("Creating Trivy scan pod...")
try:
    # Create the pod
    pod = v1.create_namespaced_pod(namespace='jenkins', body=trivy_pod)
    print(f"Trivy scan pod created: {pod.metadata.name}")
    
    # Wait for pod completion
    print("Waiting for Trivy scan to complete...")
    for i in range(30):  # 5 minute timeout
        try:
            pod_status = v1.read_namespaced_pod(name='trivy-scan-${buildNumber}', namespace='jenkins')
            phase = pod_status.status.phase
            print(f"Trivy pod status: {phase}")
            
            if phase == 'Succeeded':
                print("‚úÖ Trivy scan completed successfully")
                break
            elif phase == 'Failed':
                print("‚ùå Trivy scan failed")
                break
                
            time.sleep(10)
        except Exception as e:
            print(f"Error checking pod status: {e}")
            time.sleep(5)
    
    # Get scan results from logs
    try:
        logs = v1.read_namespaced_pod_log(name='trivy-scan-${buildNumber}', namespace='jenkins')
        print("Trivy scan results:")
        print(logs)
        
        # Write results to file
        with open('trivy-image-report.txt', 'w') as f:
            f.write("TRIVY IMAGE SCAN REPORT\\n")
            f.write("========================\\n")
            f.write(f"Image: ${fullImageName}\\n")
            f.write(f"Scan Date: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n")
            f.write(logs)
        
        print("‚úÖ Scan results saved to trivy-image-report.txt")
        
    except Exception as e:
        print(f"Error getting scan results: {e}")
        # Create placeholder report
        with open('trivy-image-report.txt', 'w') as f:
            f.write("TRIVY IMAGE SCAN REPORT\\n")
            f.write("========================\\n")
            f.write(f"Image: ${fullImageName}\\n")
            f.write("Status: Scan completed but results could not be retrieved\\n")
    
    # Cleanup pod
    try:
        v1.delete_namespaced_pod(name='trivy-scan-${buildNumber}', namespace='jenkins')
        print("Trivy scan pod cleaned up")
    except:
        pass
        
except Exception as e:
    print(f"Failed to create Trivy scan pod: {e}")
    # Create error report
    with open('trivy-image-report.txt', 'w') as f:
        f.write("TRIVY IMAGE SCAN REPORT\\n")
        f.write("========================\\n")
        f.write(f"Image: ${fullImageName}\\n")
        f.write(f"Status: Scan failed - {str(e)}\\n")

PYEOF
                    """
                }
                archiveArtifacts artifacts: 'trivy-image-report.txt', allowEmptyArchive: true
            }
        }

        stage('Deploy to Kubernetes') {
            steps {
                script {
                    def fullImageName = "${env.REGISTRY}/${env.IMAGE_NAME}:${env.TAG}"
                    def buildNumber = env.BUILD_NUMBER
                    def gitCommit = sh(script: 'git rev-parse HEAD', returnStdout: true).trim()
                    
                    // Update deployment using Python Kubernetes API
                    sh """
                        python3 << 'PYEOF'
import time
from kubernetes import client, config

# Load in-cluster config
config.load_incluster_config()

# Create API client
apps_v1 = client.AppsV1Api()
v1 = client.CoreV1Api()

print("Updating deployment with new image: ${fullImageName}")
print("Build number: ${buildNumber}")
print("Git commit: ${gitCommit}")

try:
    # Read current deployment
    deployment = apps_v1.read_namespaced_deployment(name="flask-app", namespace="flask-app")
    
    # Show current image
    current_image = deployment.spec.template.spec.containers[0].image
    print(f"Current image: {current_image}")
    print(f"New image: ${fullImageName}")
    
    # Update the image
    deployment.spec.template.spec.containers[0].image = "${fullImageName}"
    
    # Add build environment variables to the container
    if not hasattr(deployment.spec.template.spec.containers[0], 'env') or deployment.spec.template.spec.containers[0].env is None:
        deployment.spec.template.spec.containers[0].env = []
    
    # Remove existing build-related env vars to avoid duplicates
    deployment.spec.template.spec.containers[0].env = [
        env for env in deployment.spec.template.spec.containers[0].env 
        if env.name not in ['BUILD_NUMBER', 'GIT_COMMIT', 'IMAGE_TAG']
    ]
    
    # Add new build environment variables
    from kubernetes.client.models import V1EnvVar
    build_env_vars = [
        V1EnvVar(name="BUILD_NUMBER", value="${buildNumber}"),
        V1EnvVar(name="GIT_COMMIT", value="${gitCommit}"),
        V1EnvVar(name="IMAGE_TAG", value="${fullImageName.split(':')[-1]}")
    ]
    deployment.spec.template.spec.containers[0].env.extend(build_env_vars)
    
    # Force pods to be recreated by updating a timestamp annotation
    import time
    if not deployment.spec.template.metadata.annotations:
        deployment.spec.template.metadata.annotations = {}
    deployment.spec.template.metadata.annotations['deployment.kubernetes.io/restart'] = str(int(time.time()))
    
    # Apply the update
    apps_v1.patch_namespaced_deployment(
        name="flask-app", 
        namespace="flask-app", 
        body=deployment
    )
    
    print("Deployment updated, waiting for rollout to complete...")
    
    # Wait for rollout to complete
    for i in range(18):  # 3 minute timeout (reduced from 5 minutes)
        try:
            deployment = apps_v1.read_namespaced_deployment(name="flask-app", namespace="flask-app")
            
            ready_replicas = deployment.status.ready_replicas or 0
            updated_replicas = deployment.status.updated_replicas or 0
            desired_replicas = deployment.spec.replicas
            
            print(f"Rollout status: {ready_replicas}/{desired_replicas} ready, {updated_replicas} updated")
            
            if (ready_replicas == desired_replicas and updated_replicas == desired_replicas):
                print("‚úÖ Deployment rollout completed successfully")
                
                # Show pod status with detailed info
                pods = v1.list_namespaced_pod(namespace="flask-app", label_selector="app=flask-app")
                print("Pod status:")
                for pod in pods.items:
                    pod_image = pod.spec.containers[0].image if pod.spec.containers else "unknown"
                    print(f"  {pod.metadata.name}: {pod.status.phase} - Image: {pod_image}")
                
                # Verify the actual running image matches what we deployed
                running_images = set()
                for pod in pods.items:
                    if pod.spec.containers:
                        running_images.add(pod.spec.containers[0].image)
                
                if "${fullImageName}" in running_images:
                    print(f"‚úÖ Verified: Pods are running the new image: ${fullImageName}")
                else:
                    print(f"‚ö†Ô∏è  Warning: Expected image ${fullImageName} but found: {running_images}")
                
                print(f"Deployment updated successfully with image: ${fullImageName}")
                exit(0)
                
            time.sleep(10)
            
        except Exception as e:
            print(f"Error checking deployment status: {e}")
            time.sleep(5)  # Shorter sleep on error
    
    print("‚ùå Deployment rollout timed out")
    exit(1)
    
except Exception as e:
    print(f"Failed to update deployment: {e}")
    exit(1)

PYEOF
                    """
                }
            }
        }
    }

    post {
        always {
            echo 'Pipeline execution completed'
        }
        
        success {
            echo '‚úÖ Pipeline completed successfully!'
        }
        
        failure {
            echo '‚ùå Pipeline failed. Check the logs for details.'
        }
        
        unstable {
            echo '‚ö†Ô∏è Pipeline completed with warnings.'
        }
    }
}
